# {{node_name}}

{{description}}

## Quick Reference

| Item | Value |
|------|-------|
| Type | Python |
| Run | `bubbaloop node start {{node_name}}` |
| Logs | `bubbaloop node logs {{node_name}} -f` |

## File Structure

```
{{node_name}}/
├── AGENTS.md         # This file - AI agent context
├── node.yaml         # Daemon manifest (name, type, command)
├── config.yaml       # Runtime configuration
├── requirements.txt  # Python dependencies
└── main.py           # Main implementation (EDIT THIS)
```

## How to Modify

### Adding a New Zenoh Publisher
Edit `main.py`:
```python
# Topics follow scoped pattern: bubbaloop/{scope}/{machine_id}/{{node_name}}/{resource}
# The topic prefix is built automatically from BUBBALOOP_SCOPE and BUBBALOOP_MACHINE_ID
publish_topic = f"{self._topic_prefix}/my_topic"
publisher = session.declare_publisher(publish_topic)

# Publish data (in run loop)
publisher.put(json.dumps(data).encode())
```

### Adding a Subscriber
```python
def on_message(sample):
    data = json.loads(sample.payload.to_bytes())
    # Process data

# Subscribe using scoped pattern (wildcards discover across machines)
subscriber = session.declare_subscriber("bubbaloop/**/other_node/output", on_message)
```

### Changing Configuration
Edit `config.yaml`:
```yaml
publish_topic: "output"  # Appended to bubbaloop/{scope}/{machine_id}/{{node_name}}/
rate_hz: 10.0  # Adjust publish rate
```

## Zenoh Topics

All topics follow the scoped pattern: `bubbaloop/{scope}/{machine_id}/{{node_name}}/{resource}`

| Topic Suffix | Direction | Format | Description |
|--------------|-----------|--------|-------------|
| `output` | Publish | JSON/Protobuf | Main output data |
| `schema` | Queryable | Protobuf | FileDescriptorSet (REQUIRED for protobuf nodes) |
| `manifest` | Queryable | JSON | Node self-description contract |
| `health` | Queryable | JSON | Node health status |
| `config` | Queryable | JSON | Read-only configuration |

Data published to these topics is:
- Visible in the Bubbaloop Dashboard
- Available to other nodes on the Zenoh network
- Persisted if a recorder node is running

### Schema Contract (Protobuf Nodes)

If this node publishes protobuf messages, it **MUST** serve its schema:

1. **Add protos directory** with your .proto files + `header.proto`:
   ```
   {{node_name}}/
   ├── protos/
   │   ├── header.proto         # Copy from bubbaloop-schemas
   │   └── {{node_name}}.proto
   ├── descriptor.bin            # Compiled by protoc (add to .gitignore)
   └── main.py
   ```

2. **Compile descriptor.bin** using protoc:
   ```bash
   protoc --descriptor_set_out=descriptor.bin \
          --include_imports \
          protos/{{node_name}}.proto protos/header.proto
   ```

3. **Declare schema queryable** in `main.py`:
   ```python
   # Load descriptor once at startup
   with open("descriptor.bin", "rb") as f:
       DESCRIPTOR_BYTES = f.read()

   # Declare queryable (NEVER use complete=True)
   @session.declare_queryable(f"{topic_prefix}/schema")
   def handle_schema_query(query):
       # query.key_expr is a PROPERTY, not a method
       query.reply(query.key_expr, DESCRIPTOR_BYTES)
   ```

4. **Common mistakes** to avoid:
   - ❌ `complete=True` — blocks wildcard discovery
   - ❌ `query.key_expr()` — TypeError (it's a property, not method)
   - ❌ Serving JSON instead of raw bytes

See `ARCHITECTURE.md` for full schema contract details.

## Dependencies

This node has no dependencies on other nodes.
